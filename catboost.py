# -*- coding: utf-8 -*-
"""catboost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12R6uJwldF78e87Oihr8v9hu0xiRvX7d2
"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 2. Load the data (Kaggle dataset path)
# If in Kaggle, upload 'water_potability.csv' to working directory or use Kaggle datasets API
df = pd.read_csv('/content/water_potability.csv')

# 3. Quick inspection
print(df.info())
print(df.isnull().sum())

# 4. Separate features & target
X = df.drop('Potability', axis=1)
y = df['Potability']

# 5. KNN Imputation for missing values
# KNN considers similarity between samples, often better than mean/median for this dataset
imputer = KNNImputer(n_neighbors=5)
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# 6. Optional: Outlier handling
# Using IQR method to clip extreme values
def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return np.clip(series, lower, upper)

X_capped = X_imputed.apply(cap_outliers)

# 7. Feature scaling (StandardScaler)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_capped), columns=X.columns)

# 8. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 9. Final check
print("Training shape:", X_train.shape)
print("Testing shape:", X_test.shape)
print("Sample features after scaling:\n", X_train.head())

!pip install catboost

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix, classification_report
)
from catboost import CatBoostClassifier
import warnings
warnings.filterwarnings('ignore')

def load_and_preprocess_data():
    """
    Load dataset from uploaded file and preprocess it
    """
    try:
        # Try to read the uploaded file
        # Replace 'your_dataset.csv' with the actual filename
        file_path = input("/content/water_potability.csv ")

        # Determine file type and load accordingly
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):
            df = pd.read_excel(file_path)
        elif file_path.endswith('.json'):
            df = pd.read_json(file_path)
        else:
            print("Unsupported file format. Using sample dataset.")
            return create_sample_data()

        print(f"Dataset loaded successfully!")
        print(f"Shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")

        return df

    except FileNotFoundError:
        print(f"File '{file_path}' not found. Using sample dataset.")
        return create_sample_data()
    except Exception as e:
        print(f"Error loading dataset: {e}")
        print("Using sample dataset.")
        return create_sample_data()

def create_sample_data():
    """Create sample dataset if no file is uploaded"""
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=10000,
        n_features=20,
        n_informative=15,
        n_redundant=2,
        n_clusters_per_class=1,
        class_sep=1.2,
        random_state=42
    )

    # Create DataFrame
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    df = pd.DataFrame(X, columns=feature_names)
    df['target'] = y

    print("Using sample classification dataset")
    return df

def preprocess_dataset(df):
    """
    Preprocess the dataset for training
    """
    print("\n" + "="*50)
    print("DATA PREPROCESSING")
    print("="*50)

    # Display basic info
    print(f"Dataset shape: {df.shape}")
    print(f"Missing values:\n{df.isnull().sum().sum()} total missing values")

    # Ask user to specify target column
    print(f"\nAvailable columns: {list(df.columns)}")
    target_col = input("Enter the target column name (or press Enter for 'target'): ").strip()
    if not target_col:
        target_col = 'target'

    if target_col not in df.columns:
        print(f"Column '{target_col}' not found. Available columns: {list(df.columns)}")
        target_col = df.columns[-1]  # Use last column as default
        print(f"Using '{target_col}' as target column")

    # Separate features and target
    y = df[target_col].copy()
    X = df.drop(columns=[target_col])

    # Handle missing values
    if X.isnull().sum().sum() > 0:
        print("Handling missing values...")
        # Fill numeric columns with median
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())

        # Fill categorical columns with mode
        categorical_cols = X.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 'unknown')

    # Encode categorical variables
    categorical_features = []
    label_encoders = {}

    for col in X.select_dtypes(include=['object']).columns:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
        label_encoders[col] = le
        categorical_features.append(col)

    # Encode target if it's categorical
    if y.dtype == 'object' or y.dtype.name == 'category':
        le_target = LabelEncoder()
        y = le_target.fit_transform(y)
        print(f"Target classes: {le_target.classes_}")

    print(f"Features shape: {X.shape}")
    print(f"Target shape: {y.shape}")
    print(f"Categorical features: {categorical_features}")
    print(f"Class distribution: {np.bincount(y)}")

    return X, y, categorical_features

def train_catboost_model(X, y, categorical_features=None):
    """
    Train CatBoost model with optimization for high accuracy
    """
    # Set random seed for reproducibility
    np.random.seed(42)

    # Split the data with stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\nTraining set shape: {X_train.shape}")
    print(f"Test set shape: {X_test.shape}")

    # Initialize CatBoost with optimized parameters
    catboost_model = CatBoostClassifier(
        iterations=1500,
        learning_rate=0.1,
        depth=8,
        l2_leaf_reg=3,
        border_count=254,
        random_seed=42,
        verbose=200,
        early_stopping_rounds=100,
        eval_metric='Accuracy',
        cat_features=categorical_features,
        auto_class_weights='Balanced'  # Handle class imbalance
    )

    print("\n" + "="*50)
    print("TRAINING CATBOOST MODEL")
    print("="*50)

    # Train the model
    catboost_model.fit(
        X_train, y_train,
        eval_set=(X_test, y_test),
        plot=False
    )

    return catboost_model, X_train, X_test, y_train, y_test

def evaluate_model(model, X_test, y_test):
    """
    Comprehensive model evaluation
    """
    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)

    # Handle binary vs multiclass
    if len(np.unique(y_test)) == 2:
        y_pred_proba_pos = y_pred_proba[:, 1]
        average_method = 'binary'
    else:
        y_pred_proba_pos = y_pred_proba
        average_method = 'macro'

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average=average_method, zero_division=0)
    recall = recall_score(y_test, y_pred, average=average_method, zero_division=0)
    f1 = f1_score(y_test, y_pred, average=average_method, zero_division=0)

    # Calculate AUC-ROC
    try:
        if len(np.unique(y_test)) == 2:
            auc_roc = roc_auc_score(y_test, y_pred_proba_pos)
        else:
            auc_roc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
    except ValueError:
        auc_roc = np.nan

    print("\n" + "="*50)
    print("MODEL PERFORMANCE METRICS")
    print("="*50)
    print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    if not np.isnan(auc_roc):
        print(f"AUC-ROC:   {auc_roc:.4f}")

    print("\n" + "="*50)
    print("DETAILED CLASSIFICATION REPORT")
    print("="*50)
    print(classification_report(y_test, y_pred, zero_division=0))

    return y_pred, y_pred_proba, accuracy, precision, recall, f1, auc_roc

def create_visualizations(model, X_test, y_test, y_pred, y_pred_proba, auc_roc):
    """
    Create comprehensive visualizations
    """
    # Determine if binary or multiclass
    is_binary = len(np.unique(y_test)) == 2

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('CatBoost Model Evaluation', fontsize=16, fontweight='bold')

    # 1. Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
    axes[0,0].set_title('Confusion Matrix')
    axes[0,0].set_xlabel('Predicted')
    axes[0,0].set_ylabel('Actual')

    # 2. ROC Curve (only for binary classification)
    if is_binary and not np.isnan(auc_roc):
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])
        axes[0,1].plot(fpr, tpr, color='darkorange', lw=2,
                       label=f'ROC Curve (AUC = {auc_roc:.3f})')
        axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[0,1].set_xlim([0.0, 1.0])
        axes[0,1].set_ylim([0.0, 1.05])
        axes[0,1].set_xlabel('False Positive Rate')
        axes[0,1].set_ylabel('True Positive Rate')
        axes[0,1].set_title('ROC Curve')
        axes[0,1].legend(loc="lower right")
        axes[0,1].grid(True, alpha=0.3)
    else:
        axes[0,1].text(0.5, 0.5, 'ROC Curve\n(Binary Classification Only)',
                      ha='center', va='center', fontsize=12)
        axes[0,1].set_xlim([0, 1])
        axes[0,1].set_ylim([0, 1])

    # 3. Feature Importance
    try:
        feature_importance = model.get_feature_importance()
        feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=True).tail(15)

        axes[1,0].barh(importance_df['feature'], importance_df['importance'])
        axes[1,0].set_title('Top 15 Feature Importance')
        axes[1,0].set_xlabel('Importance')
    except:
        axes[1,0].text(0.5, 0.5, 'Feature Importance\nNot Available',
                      ha='center', va='center', fontsize=12)

    # 4. Prediction Distribution
    if is_binary:
        axes[1,1].hist(y_pred_proba[y_test==0, 1], bins=30, alpha=0.7,
                      label='Class 0', color='blue')
        axes[1,1].hist(y_pred_proba[y_test==1, 1], bins=30, alpha=0.7,
                      label='Class 1', color='red')
        axes[1,1].set_xlabel('Prediction Probability')
        axes[1,1].set_ylabel('Frequency')
        axes[1,1].set_title('Prediction Probability Distribution')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
    else:
        axes[1,1].text(0.5, 0.5, 'Probability Distribution\n(Binary Classification Only)',
                      ha='center', va='center', fontsize=12)

    plt.tight_layout()
    plt.show()

def hyperparameter_tuning(X_train, y_train, categorical_features=None):
    """
    Perform hyperparameter tuning for better accuracy
    """
    print("\n" + "="*50)
    print("HYPERPARAMETER TUNING")
    print("="*50)

    param_grid = {
        'depth': [6, 8, 10],
        'learning_rate': [0.05, 0.1, 0.15],
        'iterations': [1000, 1500],
        'l2_leaf_reg': [1, 3, 5]
    }

    catboost_base = CatBoostClassifier(
        random_seed=42,
        verbose=False,
        cat_features=categorical_features,
        auto_class_weights='Balanced'
    )

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    grid_search = GridSearchCV(
        catboost_base,
        param_grid,
        cv=cv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )

    print("Starting grid search... This may take several minutes.")
    grid_search.fit(X_train, y_train)

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best CV accuracy: {grid_search.best_score_:.4f}")

    return grid_search.best_estimator_

# Main execution
def main():
    print("="*60)
    print("CATBOOST MODEL TRAINING WITH UPLOADED DATASET")
    print("="*60)

    # Load and preprocess data
    df = load_and_preprocess_data()
    X, y, categorical_features = preprocess_dataset(df)

    # Train initial model
    model, X_train, X_test, y_train, y_test = train_catboost_model(X, y, categorical_features)

    # Evaluate model
    y_pred, y_pred_proba, accuracy, precision, recall, f1, auc_roc = evaluate_model(model, X_test, y_test)

    # Create visualizations
    create_visualizations(model, X_test, y_test, y_pred, y_pred_proba, auc_roc)

    # Ask for hyperparameter tuning if accuracy is below 95%
    if accuracy < 0.95:
        tune_hyperparams = input(f"\nCurrent accuracy: {accuracy:.4f}. Would you like to perform hyperparameter tuning to improve accuracy? (y/n): ").lower().strip()

        if tune_hyperparams == 'y':
            best_model = hyperparameter_tuning(X_train, y_train, categorical_features)
            print("\nRe-evaluating with best parameters...")
            y_pred_tuned, y_pred_proba_tuned, accuracy_tuned, precision_tuned, recall_tuned, f1_tuned, auc_roc_tuned = evaluate_model(best_model, X_test, y_test)

            if accuracy_tuned > accuracy:
                print(f"Improved accuracy: {accuracy:.4f} -> {accuracy_tuned:.4f}")
                create_visualizations(best_model, X_test, y_test, y_pred_tuned, y_pred_proba_tuned, auc_roc_tuned)
            else:
                print("No significant improvement from hyperparameter tuning.")

    # Tips for achieving 95%+ accuracy
    print("\n" + "="*60)
    print("TIPS TO ACHIEVE 95%+ ACCURACY:")
    print("="*60)
    print("1. Feature Engineering: Create domain-specific features")
    print("2. Data Quality: Clean outliers, handle missing values carefully")
    print("3. Feature Selection: Remove irrelevant/redundant features")
    print("4. Cross-Validation: Use stratified k-fold validation")
    print("5. Ensemble Methods: Combine multiple models")
    print("6. Data Augmentation: Increase training data if possible")
    print("7. Class Balancing: Handle imbalanced datasets properly")
    print("8. Domain Knowledge: Incorporate expert knowledge into features")

    print("\n" + "="*60)
    print("MODEL TRAINING COMPLETED!")
    print("="*60)

# Run the main function
if __name__ == "__main__":
    main()