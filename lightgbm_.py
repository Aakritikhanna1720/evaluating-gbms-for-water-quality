# -*- coding: utf-8 -*-
"""lightgbm .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-8JccbPS-azoqRgc6cT2jmcbsTzUO6us
"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 2. Load the data (Kaggle dataset path)
# If in Kaggle, upload 'water_potability.csv' to working directory or use Kaggle datasets API
df = pd.read_csv('/content/water_potability.csv')

# 3. Quick inspection
print(df.info())
print(df.isnull().sum())

# 4. Separate features & target
X = df.drop('Potability', axis=1)
y = df['Potability']

# 5. KNN Imputation for missing values
# KNN considers similarity between samples, often better than mean/median for this dataset
imputer = KNNImputer(n_neighbors=5)
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# 6. Optional: Outlier handling
# Using IQR method to clip extreme values
def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return np.clip(series, lower, upper)

X_capped = X_imputed.apply(cap_outliers)

# 7. Feature scaling (StandardScaler)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_capped), columns=X.columns)

# 8. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 9. Final check
print("Training shape:", X_train.shape)
print("Testing shape:", X_test.shape)
print("Sample features after scaling:\n", X_train.head())

!pip install lightgbm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix, classification_report
)
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

def load_and_preprocess_data():
    """
    Load dataset from uploaded file and preprocess it
    """
    try:
        # Try to read the uploaded file
        file_path = input("/content/water_potability.csv'): ")

        # Determine file type and load accordingly
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):
            df = pd.read_excel(file_path)
        elif file_path.endswith('.json'):
            df = pd.read_json(file_path)
        else:
            print("Unsupported file format. Using sample dataset.")
            return create_sample_data()

        print(f"Dataset loaded successfully!")
        print(f"Shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")

        return df

    except FileNotFoundError:
        print(f"File '{file_path}' not found. Using sample dataset.")
        return create_sample_data()
    except Exception as e:
        print(f"Error loading dataset: {e}")
        print("Using sample dataset.")
        return create_sample_data()

def create_sample_data():
    """Create sample dataset if no file is uploaded"""
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=10000,
        n_features=20,
        n_informative=15,
        n_redundant=2,
        n_clusters_per_class=1,
        class_sep=1.2,
        random_state=42
    )

    # Create DataFrame
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    df = pd.DataFrame(X, columns=feature_names)
    df['target'] = y

    print("Using sample classification dataset")
    return df

def preprocess_dataset(df):
    """
    Preprocess the dataset for training
    """
    print("\n" + "="*50)
    print("DATA PREPROCESSING")
    print("="*50)

    # Display basic info
    print(f"Dataset shape: {df.shape}")
    print(f"Missing values:\n{df.isnull().sum().sum()} total missing values")

    # Ask user to specify target column
    print(f"\nAvailable columns: {list(df.columns)}")
    target_col = input("Enter the target column name (or press Enter for 'target'): ").strip()
    if not target_col:
        target_col = 'target'

    if target_col not in df.columns:
        print(f"Column '{target_col}' not found. Available columns: {list(df.columns)}")
        target_col = df.columns[-1]  # Use last column as default
        print(f"Using '{target_col}' as target column")

    # Separate features and target
    y = df[target_col].copy()
    X = df.drop(columns=[target_col])

    # Handle missing values
    if X.isnull().sum().sum() > 0:
        print("Handling missing values...")
        # Fill numeric columns with median
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())

        # Fill categorical columns with mode
        categorical_cols = X.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 'unknown')

    # Encode categorical variables
    categorical_features = []
    label_encoders = {}

    for col in X.select_dtypes(include=['object']).columns:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
        label_encoders[col] = le
        categorical_features.append(col)

    # Encode target if it's categorical
    if y.dtype == 'object' or y.dtype.name == 'category':
        le_target = LabelEncoder()
        y = le_target.fit_transform(y)
        print(f"Target classes: {le_target.classes_}")

    print(f"Features shape: {X.shape}")
    print(f"Target shape: {y.shape}")
    print(f"Categorical features: {categorical_features}")
    print(f"Class distribution: {np.bincount(y)}")

    return X, y, categorical_features

def train_lightgbm_model(X, y, categorical_features=None):
    """
    Train LightGBM model with optimization for high accuracy
    """
    # Set random seed for reproducibility
    np.random.seed(42)

    # Split the data with stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\nTraining set shape: {X_train.shape}")
    print(f"Test set shape: {X_test.shape}")

    # Prepare LightGBM datasets
    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data, categorical_feature=categorical_features)

    # LightGBM parameters optimized for high accuracy
    params = {
        'objective': 'binary' if len(np.unique(y)) == 2 else 'multiclass',
        'num_class': len(np.unique(y)) if len(np.unique(y)) > 2 else None,
        'boosting_type': 'gbdt',
        'metric': 'binary_logloss' if len(np.unique(y)) == 2 else 'multi_logloss',
        'num_leaves': 31,
        'learning_rate': 0.1,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_child_samples': 20,
        'min_child_weight': 0.001,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'random_state': 42,
        'n_jobs': -1,
        'verbose': -1,
        'is_unbalance': True,  # Handle class imbalance
        'max_depth': 8
    }

    print("\n" + "="*50)
    print("TRAINING LIGHTGBM MODEL")
    print("="*50)

    # Train the model with early stopping
    callbacks = [lgb.log_evaluation(period=100), lgb.early_stopping(stopping_rounds=100)]

    lgb_model = lgb.train(
        params,
        train_data,
        valid_sets=[train_data, valid_data],
        valid_names=['train', 'eval'],
        num_boost_round=2000,
        callbacks=callbacks
    )

    return lgb_model, X_train, X_test, y_train, y_test, train_data, valid_data

def evaluate_model(model, X_test, y_test):
    """
    Comprehensive model evaluation
    """
    # Make predictions
    y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)

    # Handle binary vs multiclass
    if len(np.unique(y_test)) == 2:
        y_pred = (y_pred_proba > 0.5).astype(int)
        y_pred_proba_pos = y_pred_proba
        average_method = 'binary'
    else:
        y_pred = np.argmax(y_pred_proba, axis=1)
        y_pred_proba_pos = y_pred_proba
        average_method = 'macro'

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average=average_method, zero_division=0)
    recall = recall_score(y_test, y_pred, average=average_method, zero_division=0)
    f1 = f1_score(y_test, y_pred, average=average_method, zero_division=0)

    # Calculate AUC-ROC
    try:
        if len(np.unique(y_test)) == 2:
            auc_roc = roc_auc_score(y_test, y_pred_proba_pos)
        else:
            auc_roc = roc_auc_score(y_test, y_pred_proba_pos, multi_class='ovr')
    except ValueError:
        auc_roc = np.nan

    print("\n" + "="*50)
    print("MODEL PERFORMANCE METRICS")
    print("="*50)
    print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    if not np.isnan(auc_roc):
        print(f"AUC-ROC:   {auc_roc:.4f}")

    print("\n" + "="*50)
    print("DETAILED CLASSIFICATION REPORT")
    print("="*50)
    print(classification_report(y_test, y_pred, zero_division=0))

    return y_pred, y_pred_proba, accuracy, precision, recall, f1, auc_roc

def create_visualizations(model, X_test, y_test, y_pred, y_pred_proba, auc_roc):
    """
    Create comprehensive visualizations
    """
    # Determine if binary or multiclass
    is_binary = len(np.unique(y_test)) == 2

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('LightGBM Model Evaluation', fontsize=16, fontweight='bold')

    # 1. Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
    axes[0,0].set_title('Confusion Matrix')
    axes[0,0].set_xlabel('Predicted')
    axes[0,0].set_ylabel('Actual')

    # 2. ROC Curve (only for binary classification)
    if is_binary and not np.isnan(auc_roc):
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
        axes[0,1].plot(fpr, tpr, color='darkorange', lw=2,
                       label=f'ROC Curve (AUC = {auc_roc:.3f})')
        axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[0,1].set_xlim([0.0, 1.0])
        axes[0,1].set_ylim([0.0, 1.05])
        axes[0,1].set_xlabel('False Positive Rate')
        axes[0,1].set_ylabel('True Positive Rate')
        axes[0,1].set_title('ROC Curve')
        axes[0,1].legend(loc="lower right")
        axes[0,1].grid(True, alpha=0.3)
    else:
        axes[0,1].text(0.5, 0.5, 'ROC Curve\n(Binary Classification Only)',
                      ha='center', va='center', fontsize=12)
        axes[0,1].set_xlim([0, 1])
        axes[0,1].set_ylim([0, 1])

    # 3. Feature Importance
    try:
        feature_importance = model.feature_importance(importance_type='gain')
        feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=True).tail(15)

        axes[0,2].barh(importance_df['feature'], importance_df['importance'])
        axes[0,2].set_title('Top 15 Feature Importance (Gain)')
        axes[0,2].set_xlabel('Importance')
    except:
        axes[0,2].text(0.5, 0.5, 'Feature Importance\nNot Available',
                      ha='center', va='center', fontsize=12)

    # 4. Training History
    try:
        # Get training history
        train_scores = model.evals_result_['train']['binary_logloss'] if is_binary else model.evals_result_['train']['multi_logloss']
        eval_scores = model.evals_result_['eval']['binary_logloss'] if is_binary else model.evals_result_['eval']['multi_logloss']

        axes[1,0].plot(train_scores, label='Training Loss', color='blue')
        axes[1,0].plot(eval_scores, label='Validation Loss', color='red')
        axes[1,0].set_xlabel('Iteration')
        axes[1,0].set_ylabel('Loss')
        axes[1,0].set_title('Training History')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)

        # Mark best iteration
        if hasattr(model, 'best_iteration'):
            axes[1,0].axvline(x=model.best_iteration, color='green', linestyle='--',
                             label=f'Best Iteration: {model.best_iteration}')
            axes[1,0].legend()
    except:
        axes[1,0].text(0.5, 0.5, 'Training History\nNot Available',
                      ha='center', va='center', fontsize=12)

    # 5. Prediction Distribution
    if is_binary:
        axes[1,1].hist(y_pred_proba[y_test==0], bins=30, alpha=0.7,
                      label='Class 0', color='blue')
        axes[1,1].hist(y_pred_proba[y_test==1], bins=30, alpha=0.7,
                      label='Class 1', color='red')
        axes[1,1].set_xlabel('Prediction Probability')
        axes[1,1].set_ylabel('Frequency')
        axes[1,1].set_title('Prediction Probability Distribution')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
    else:
        axes[1,1].text(0.5, 0.5, 'Probability Distribution\n(Binary Classification Only)',
                      ha='center', va='center', fontsize=12)

    # 6. Feature Importance (Split)
    try:
        feature_importance_split = model.feature_importance(importance_type='split')
        importance_df_split = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance_split
        }).sort_values('importance', ascending=True).tail(15)

        axes[1,2].barh(importance_df_split['feature'], importance_df_split['importance'])
        axes[1,2].set_title('Top 15 Feature Importance (Split)')
        axes[1,2].set_xlabel('Number of Splits')
    except:
        axes[1,2].text(0.5, 0.5, 'Feature Importance (Split)\nNot Available',
                      ha='center', va='center', fontsize=12)

    plt.tight_layout()
    plt.show()

def hyperparameter_tuning(X_train, y_train, categorical_features=None):
    """
    Perform hyperparameter tuning for better accuracy using LightGBM
    """
    print("\n" + "="*50)
    print("HYPERPARAMETER TUNING")
    print("="*50)

    # Create LightGBM classifier for sklearn interface
    lgb_classifier = lgb.LGBMClassifier(
        random_state=42,
        n_jobs=-1,
        verbose=-1,
        is_unbalance=True
    )

    param_grid = {
        'num_leaves': [20, 31, 40],
        'learning_rate': [0.05, 0.1, 0.15],
        'n_estimators': [100, 200, 300],
        'max_depth': [6, 8, 10],
        'min_child_samples': [10, 20, 30],
        'reg_alpha': [0.0, 0.1, 0.2],
        'reg_lambda': [0.0, 0.1, 0.2]
    }

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    grid_search = GridSearchCV(
        lgb_classifier,
        param_grid,
        cv=cv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )

    print("Starting grid search... This may take several minutes.")
    grid_search.fit(X_train, y_train)

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best CV accuracy: {grid_search.best_score_:.4f}")

    return grid_search.best_estimator_

def evaluate_sklearn_model(model, X_test, y_test):
    """
    Evaluate sklearn interface LightGBM model
    """
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)

    # Handle binary vs multiclass
    if len(np.unique(y_test)) == 2:
        y_pred_proba_pos = y_pred_proba[:, 1]
        average_method = 'binary'
    else:
        y_pred_proba_pos = y_pred_proba
        average_method = 'macro'

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average=average_method, zero_division=0)
    recall = recall_score(y_test, y_pred, average=average_method, zero_division=0)
    f1 = f1_score(y_test, y_pred, average=average_method, zero_division=0)

    # Calculate AUC-ROC
    try:
        if len(np.unique(y_test)) == 2:
            auc_roc = roc_auc_score(y_test, y_pred_proba_pos)
        else:
            auc_roc = roc_auc_score(y_test, y_pred_proba_pos, multi_class='ovr')
    except ValueError:
        auc_roc = np.nan

    print("\nTuned Model Performance:")
    print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    if not np.isnan(auc_roc):
        print(f"AUC-ROC:   {auc_roc:.4f}")

    return y_pred, y_pred_proba, accuracy, precision, recall, f1, auc_roc

# Main execution
def main():
    print("="*60)
    print("LIGHTGBM MODEL TRAINING WITH UPLOADED DATASET")
    print("="*60)

    # Load and preprocess data
    df = load_and_preprocess_data()
    X, y, categorical_features = preprocess_dataset(df)

    # Train initial model
    model, X_train, X_test, y_train, y_test, train_data, valid_data = train_lightgbm_model(X, y, categorical_features)

    # Evaluate model
    y_pred, y_pred_proba, accuracy, precision, recall, f1, auc_roc = evaluate_model(model, X_test, y_test)

    # Create visualizations
    create_visualizations(model, X_test, y_test, y_pred, y_pred_proba, auc_roc)

    # Ask for hyperparameter tuning if accuracy is below 95%
    if accuracy < 0.95:
        tune_hyperparams = input(f"\nCurrent accuracy: {accuracy:.4f}. Would you like to perform hyperparameter tuning to improve accuracy? (y/n): ").lower().strip()

        if tune_hyperparams == 'y':
            best_model = hyperparameter_tuning(X_train, y_train, categorical_features)
            print("\nRe-evaluating with best parameters...")
            y_pred_tuned, y_pred_proba_tuned, accuracy_tuned, precision_tuned, recall_tuned, f1_tuned, auc_roc_tuned = evaluate_sklearn_model(best_model, X_test, y_test)

            if accuracy_tuned > accuracy:
                print(f"Improved accuracy: {accuracy:.4f} -> {accuracy_tuned:.4f}")
                # Note: Visualization for tuned model would need sklearn interface adaptation
            else:
                print("No significant improvement from hyperparameter tuning.")

    # Advanced LightGBM Tips
    print("\n" + "="*60)
    print("LIGHTGBM OPTIMIZATION TIPS FOR 95%+ ACCURACY:")
    print("="*60)
    print("1. Feature Engineering: Create interaction features, polynomial features")
    print("2. Hyperparameter Tuning: Use Optuna or Bayesian optimization")
    print("3. Cross-Validation: Use stratified k-fold with early stopping")
    print("4. Feature Selection: Use LightGBM's built-in feature importance")
    print("5. Ensemble Methods: Combine with XGBoost, Random Forest")
    print("6. Data Preprocessing: Handle outliers, feature scaling if needed")
    print("7. Class Balancing: Use class_weight, SMOTE, or focal loss")
    print("8. Early Stopping: Prevent overfitting with validation set")
    print("9. Categorical Features: Use LightGBM's native categorical support")
    print("10. Boosting Strategy: Try dart or goss boosting types")

    # Model saving tip
    print(f"\nModel Info:")
    print(f"- Best iteration: {model.best_iteration}")
    print(f"- Number of features: {model.num_feature()}")
    print(f"- Objective: {model.params.get('objective', 'N/A')}")

    print("\n" + "="*60)
    print("MODEL TRAINING COMPLETED!")
    print("="*60)

    # Save model option
    save_model = input("Would you like to save the trained model? (y/n): ").lower().strip()
    if save_model == 'y':
        model_filename = input("Enter filename for the model (e.g., 'lightgbm_model.txt'): ").strip()
        if not model_filename:
            model_filename = 'lightgbm_model.txt'
        model.save_model(model_filename)
        print(f"Model saved as '{model_filename}'")

# Run the main function
if __name__ == "__main__":
    main()