# -*- coding: utf-8 -*-
"""xgboost .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lt4RuA_ZoWyn2I0kdBraHz6okj9MCkrG
"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 2. Load the data (Kaggle dataset path)
# If in Kaggle, upload 'water_potability.csv' to working directory or use Kaggle datasets API
df = pd.read_csv('/content/water_potability.csv')

# 3. Quick inspection
print(df.info())
print(df.isnull().sum())

# 4. Separate features & target
X = df.drop('Potability', axis=1)
y = df['Potability']

# 5. KNN Imputation for missing values
# KNN considers similarity between samples, often better than mean/median for this dataset
imputer = KNNImputer(n_neighbors=5)
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# 6. Optional: Outlier handling
# Using IQR method to clip extreme values
def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return np.clip(series, lower, upper)

X_capped = X_imputed.apply(cap_outliers)

# 7. Feature scaling (StandardScaler)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_capped), columns=X.columns)

# 8. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 9. Final check
print("Training shape:", X_train.shape)
print("Testing shape:", X_test.shape)
print("Sample features after scaling:\n", X_train.head())

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 2. Load the data (Kaggle dataset path)
# If in Kaggle, upload 'water_potability.csv' to working directory or use Kaggle datasets API
df = pd.read_csv('/content/water_potability.csv')

# 3. Quick inspection
print(df.info())
print(df.isnull().sum())

# 4. Separate features & target
X = df.drop('Potability', axis=1)
y = df['Potability']

# 5. KNN Imputation for missing values
# KNN considers similarity between samples, often better than mean/median for this dataset
imputer = KNNImputer(n_neighbors=5)
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
# 6. Optional: Outlier handling
# Using IQR method to clip extreme values
def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return np.clip(series, lower, upper)

X_capped = X_imputed.apply(cap_outliers)

# 7. Feature scaling (StandardScaler)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_capped), columns=X.columns)

# 8. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 9. Final check
print("Training shape:", X_train.shape)
print("Testing shape:", X_test.shape)
print("Sample features after scaling:\n", X_train.head())

# Water Quality Prediction and Potability Classification using XGBoost
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                           f1_score, roc_auc_score, classification_report,
                           confusion_matrix, roc_curve, precision_recall_curve)
from sklearn.impute import SimpleImputer
import xgboost as xgb
from xgboost import XGBClassifier
import joblib
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

print("XGBoost Water Quality Prediction System")
print("="*50)

# =============================================================================
# 1. DATA LOADING AND PREPARATION
# =============================================================================

def load_water_quality_data():
    """
    Load water quality dataset. Replace this with your actual dataset.
    This creates a synthetic dataset similar to real water quality data.
    """
    # If you have a real dataset, use:
    # df = pd.read_csv('/content/water_potability.csv')

    # For demonstration, creating synthetic water quality data
    np.random.seed(42)
    n_samples = 5000

    # Water quality features based on WHO standards
    data = {
        'ph': np.random.normal(7.2, 1.2, n_samples),
        'Hardness': np.random.normal(180, 60, n_samples),
        'Solids': np.random.normal(22000, 8000, n_samples),
        'Chloramines': np.random.normal(6.5, 2.5, n_samples),
        'Sulfate': np.random.normal(240, 80, n_samples),
        'Conductivity': np.random.normal(420, 120, n_samples),
        'Organic_carbon': np.random.normal(15, 4, n_samples),
        'Trihalomethanes': np.random.normal(70, 25, n_samples),
        'Turbidity': np.random.normal(3.8, 1.5, n_samples),
        'Temperature': np.random.normal(20, 5, n_samples),  # Additional feature
        'Iron': np.random.normal(0.15, 0.08, n_samples),   # Additional feature
    }

    df = pd.DataFrame(data)

    # Ensure realistic ranges
    df['ph'] = np.clip(df['ph'], 4, 10)
    df['Hardness'] = np.clip(df['Hardness'], 50, 500)
    df['Solids'] = np.clip(df['Solids'], 5000, 50000)
    df['Chloramines'] = np.clip(df['Chloramines'], 0, 15)
    df['Sulfate'] = np.clip(df['Sulfate'], 50, 500)
    df['Conductivity'] = np.clip(df['Conductivity'], 100, 800)
    df['Organic_carbon'] = np.clip(df['Organic_carbon'], 5, 25)
    df['Trihalomethanes'] = np.clip(df['Trihalomethanes'], 10, 150)
    df['Turbidity'] = np.clip(df['Turbidity'], 0.5, 10)
    df['Temperature'] = np.clip(df['Temperature'], 5, 35)
    df['Iron'] = np.clip(df['Iron'], 0, 1)

    # Create potability target based on water quality standards
    potability = []
    for i in range(n_samples):
        score = 0
        # pH should be between 6.5-8.5
        if 6.5 <= df.loc[i, 'ph'] <= 8.5:
            score += 2
        # Hardness should be < 300
        if df.loc[i, 'Hardness'] < 300:
            score += 1
        # Chloramines should be < 4
        if df.loc[i, 'Chloramines'] < 4:
            score += 2
        # Sulfate should be < 250
        if df.loc[i, 'Sulfate'] < 250:
            score += 1
        # Turbidity should be < 4
        if df.loc[i, 'Turbidity'] < 4:
            score += 2
        # Iron should be < 0.3
        if df.loc[i, 'Iron'] < 0.3:
            score += 1
        # Temperature should be reasonable
        if 10 <= df.loc[i, 'Temperature'] <= 30:
            score += 1

        # Water is potable if it meets sufficient criteria
        potability.append(1 if score >= 6 else 0)

    df['Potability'] = potability

    # Add some missing values to simulate real-world data
    missing_indices = np.random.choice(df.index, size=int(0.08 * len(df)), replace=False)
    for idx in missing_indices:
        col = np.random.choice(df.columns[:-1])  # Don't add missing values to target
        df.loc[idx, col] = np.nan

    return df

# Load the data
print("Loading water quality dataset...")
df = load_water_quality_data()

# Display basic information about the dataset
print(f"Dataset shape: {df.shape}")
print(f"Features: {list(df.columns[:-1])}")
print(f"Target variable: {df.columns[-1]}")
print("\nTarget distribution:")
print(df['Potability'].value_counts())
print(f"Potability rate: {df['Potability'].mean():.2%}")

# =============================================================================
# 2. EXPLORATORY DATA ANALYSIS
# =============================================================================

def plot_eda(df):
    """Comprehensive Exploratory Data Analysis"""
    fig, axes = plt.subplots(3, 2, figsize=(15, 18))

    # Target distribution
    df['Potability'].value_counts().plot(kind='bar', ax=axes[0,0],
                                        color=['lightcoral', 'lightgreen'])
    axes[0,0].set_title('Water Potability Distribution')
    axes[0,0].set_xlabel('Potability (0: Not Potable, 1: Potable)')
    axes[0,0].set_ylabel('Count')
    axes[0,0].tick_params(axis='x', rotation=0)

    # Correlation heatmap
    correlation_matrix = df.corr()
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r',
                center=0, ax=axes[0,1], fmt='.2f')
    axes[0,1].set_title('Feature Correlation Matrix')

    # Missing values
    missing_data = df.isnull().sum()
    if missing_data.sum() > 0:
        missing_data[missing_data > 0].plot(kind='bar', ax=axes[1,0], color='orange')
        axes[1,0].set_title('Missing Values by Feature')
        axes[1,0].set_ylabel('Count')
        axes[1,0].tick_params(axis='x', rotation=45)
    else:
        axes[1,0].text(0.5, 0.5, 'No Missing Values', ha='center', va='center',
                      transform=axes[1,0].transAxes, fontsize=14)
        axes[1,0].set_title('Missing Values Check')

    # Feature distributions by potability
    important_features = ['ph', 'Hardness', 'Chloramines', 'Turbidity']
    # Plot each feature's distribution by potability in a separate subplot
    for i, feature in enumerate(important_features):
        row = (i // 2) + 1
        col = i % 2
        # Ensure we are within the defined axes range
        if row < 3 and col < 2:
            sns.boxplot(data=df, x='Potability', y=feature, ax=axes[row, col])
            axes[row, col].set_title(f'{feature} Distribution by Potability')
            axes[row, col].set_xlabel('Potability')
            axes[row, col].set_xticks([0, 1])
            axes[row, col].set_xticklabels(['Not Potable', 'Potable'])


    # Hide the unused axes if there are any
    for j in range(i + 1, 4):
        row = (j // 2) + 1
        col = j % 2
        if row < 3 and col < 2:
            fig.delaxes(axes[row, col])


    plt.tight_layout()
    plt.show()

    # Feature statistics
    print("\nFeature Statistics:")
    print(df.describe())

# Perform EDA
plot_eda(df)

# =============================================================================
# 3. DATA PREPROCESSING
# =============================================================================

def preprocess_data(df):
    """Comprehensive data preprocessing"""
    # Separate features and target
    X = df.drop('Potability', axis=1)
    y = df['Potability']

    # Handle missing values
    print("Handling missing values...")
    imputer = SimpleImputer(strategy='median')
    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

    # Optional: Feature scaling (XGBoost doesn't require it, but can help)
    # Uncomment if you want to scale features
    # scaler = StandardScaler()
    # X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
    # return X_scaled, y, scaler, imputer

    return X, y, None, imputer

# Preprocess the data
X, y, scaler, imputer = preprocess_data(df)
print("Data preprocessing completed.")

# Split the data with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")
print(f"Training set potability rate: {y_train.mean():.2%}")
print(f"Test set potability rate: {y_test.mean():.2%}")

# =============================================================================
# 4. XGBOOST MODEL TRAINING
# =============================================================================

def train_xgboost_basic(X_train, y_train):
    """Train basic XGBoost model"""
    print("\nTraining basic XGBoost model...")

    xgb_basic = XGBClassifier(
        random_state=42,
        eval_metric='logloss',  # Suppress warning
        verbosity=0  # Reduce output
    )

    xgb_basic.fit(X_train, y_train)
    return xgb_basic

def train_xgboost_tuned(X_train, y_train, method='random'):
    """Train XGBoost with hyperparameter tuning"""
    print(f"\nPerforming hyperparameter tuning using {method} search...")

    # Define parameter space
    param_grid = {
        'n_estimators': [100, 200, 300, 500],
        'max_depth': [3, 4, 5, 6, 7],
        'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
        'subsample': [0.8, 0.9, 1.0],
        'colsample_bytree': [0.8, 0.9, 1.0],
        'min_child_weight': [1, 3, 5],
        'gamma': [0, 0.1, 0.2],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [1, 1.5, 2]
    }

    # For RandomizedSearch (faster)
    param_dist = {
        'n_estimators': [100, 200, 300, 400, 500],
        'max_depth': range(3, 8),
        'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
        'min_child_weight': range(1, 6),
        'gamma': [0, 0.1, 0.2, 0.3],
        'reg_alpha': [0, 0.1, 0.5, 1],
        'reg_lambda': [1, 1.5, 2, 3]
    }

    xgb_model = XGBClassifier(
        random_state=42,
        eval_metric='logloss',
        verbosity=0
    )

    if method == 'grid':
        # Grid Search (more thorough but slower)
        search = GridSearchCV(
            xgb_model,
            param_grid,
            cv=5,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=1
        )
    else:
        # Random Search (faster)
        search = RandomizedSearchCV(
            xgb_model,
            param_dist,
            n_iter=50,  # Number of parameter settings sampled
            cv=5,
            scoring='roc_auc',
            n_jobs=-1,
            random_state=42,
            verbose=1
        )

    search.fit(X_train, y_train)

    print(f"Best parameters: {search.best_params_}")
    print(f"Best cross-validation AUC: {search.best_score_:.4f}")

    return search.best_estimator_

# Train models
basic_xgb = train_xgboost_basic(X_train, y_train)
tuned_xgb = train_xgboost_tuned(X_train, y_train, method='random')

# =============================================================================
# 5. COMPREHENSIVE MODEL EVALUATION
# =============================================================================

def evaluate_xgboost_model(model, X_test, y_test, X_train, y_train, model_name="XGBoost"):
    """Comprehensive XGBoost model evaluation"""

    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Calculate all requested metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    auc = roc_auc_score(y_test, y_pred_proba)

    # Binary classification specific metrics
    precision_binary = precision_score(y_test, y_pred)
    recall_binary = recall_score(y_test, y_pred)
    f1_binary = f1_score(y_test, y_pred)

    # Cross-validation scores
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    cv_auc_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')

    print("=" * 70)
    print(f"WATER QUALITY PREDICTION - {model_name.upper()} EVALUATION RESULTS")
    print("=" * 70)
    print("MODEL PARAMETERS:")
    print(f"n_estimators: {model.n_estimators}")
    print(f"max_depth: {model.max_depth}")
    print(f"learning_rate: {model.learning_rate}")
    print(f"subsample: {model.subsample}")
    print(f"colsample_bytree: {model.colsample_bytree}")
    print("-" * 70)
    print("PERFORMANCE METRICS:")
    print(f"Accuracy:                {accuracy:.4f}")
    print(f"Precision (Weighted):    {precision:.4f}")
    print(f"Recall (Weighted):       {recall:.4f}")
    print(f"F1-Score (Weighted):     {f1:.4f}")
    print(f"AUC-ROC:                 {auc:.4f}")
    print("-" * 70)
    print("BINARY CLASSIFICATION METRICS:")
    print(f"Precision (Potable):     {precision_binary:.4f}")
    print(f"Recall (Potable):        {recall_binary:.4f}")
    print(f"F1-Score (Potable):      {f1_binary:.4f}")
    print("-" * 70)
    print("CROSS-VALIDATION RESULTS:")
    print(f"CV Accuracy:             {cv_scores.mean():.4f} (±{cv_scores.std()*2:.4f})")
    print(f"CV AUC:                  {cv_auc_scores.mean():.4f} (±{cv_auc_scores.std()*2:.4f})")
    print("-" * 70)

    # Detailed classification report
    print("DETAILED CLASSIFICATION REPORT:")
    print(classification_report(y_test, y_pred,
                              target_names=['Not Potable', 'Potable']))

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'auc': auc,
        'precision_binary': precision_binary,
        'recall_binary': recall_binary,
        'f1_binary': f1_binary,
        'cv_accuracy': cv_scores.mean(),
        'cv_auc': cv_auc_scores.mean(),
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }

# Evaluate both models
print("\n" + "="*50)
print("MODEL EVALUATION")
print("="*50)

basic_results = evaluate_xgboost_model(basic_xgb, X_test, y_test, X_train, y_train, "Basic XGBoost")
tuned_results = evaluate_xgboost_model(tuned_xgb, X_test, y_test, X_train, y_train, "Tuned XGBoost")

# =============================================================================
# 6. FEATURE IMPORTANCE ANALYSIS
# =============================================================================

def plot_feature_importance(model, feature_names, top_n=10):
    """Plot feature importance from XGBoost model"""

    # Get feature importance
    importance_gain = model.feature_importances_
    importance_weight = model.get_booster().get_score(importance_type='weight')
    importance_cover = model.get_booster().get_score(importance_type='cover')

    # Create DataFrame for easier plotting
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'gain': importance_gain,
        'weight': [importance_weight.get(f'f{i}', 0) for i in range(len(feature_names))],
        'cover': [importance_cover.get(f'f{i}', 0) for i in range(len(feature_names))]
    })

    importance_df = importance_df.sort_values('gain', ascending=False)

    # Plot top features
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # Gain importance
    top_gain = importance_df.head(top_n)
    axes[0].barh(range(len(top_gain)), top_gain['gain'], color='skyblue')
    axes[0].set_yticks(range(len(top_gain)))
    axes[0].set_yticklabels(top_gain['feature'])
    axes[0].set_xlabel('Gain')
    axes[0].set_title('Feature Importance (Gain)')
    axes[0].invert_yaxis()

    # Weight importance
    importance_df_weight = importance_df.sort_values('weight', ascending=False)
    top_weight = importance_df_weight.head(top_n)
    axes[1].barh(range(len(top_weight)), top_weight['weight'], color='lightgreen')
    axes[1].set_yticks(range(len(top_weight)))
    axes[1].set_yticklabels(top_weight['feature'])
    axes[1].set_xlabel('Weight (Frequency)')
    axes[1].set_title('Feature Importance (Weight)')
    axes[1].invert_yaxis()

    # Cover importance
    importance_df_cover = importance_df.sort_values('cover', ascending=False)
    top_cover = importance_df_cover.head(top_n)
    axes[2].barh(range(len(top_cover)), top_cover['cover'], color='lightcoral')
    axes[2].set_yticks(range(len(top_cover)))
    axes[2].set_yticklabels(top_cover['feature'])
    axes[2].set_xlabel('Cover')
    axes[2].set_title('Feature Importance (Cover)')
    axes[2].invert_yaxis()

    plt.tight_layout()
    plt.show()

    return importance_df

# Plot feature importance for tuned model
print("\nFEATURE IMPORTANCE ANALYSIS")
print("="*50)
importance_df = plot_feature_importance(tuned_xgb, X.columns, top_n=len(X.columns))
print("Top 5 Most Important Features (by Gain):")
print(importance_df[['feature', 'gain']].head())

# =============================================================================
# 7. VISUALIZATION OF RESULTS
# =============================================================================

def plot_comprehensive_results(y_test, basic_results, tuned_results):
    """Plot comprehensive evaluation results"""

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # Confusion Matrices
    cm_basic = confusion_matrix(y_test, basic_results['y_pred'])
    cm_tuned = confusion_matrix(y_test, tuned_results['y_pred'])

    sns.heatmap(cm_basic, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
    axes[0,0].set_title('Confusion Matrix - Basic XGBoost')
    axes[0,0].set_ylabel('Actual')
    axes[0,0].set_xlabel('Predicted')

    sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Greens', ax=axes[0,1])
    axes[0,1].set_title('Confusion Matrix - Tuned XGBoost')
    axes[0,1].set_ylabel('Actual')
    axes[0,1].set_xlabel('Predicted')

    # ROC Curves
    fpr_basic, tpr_basic, _ = roc_curve(y_test, basic_results['y_pred_proba'])
    fpr_tuned, tpr_tuned, _ = roc_curve(y_test, tuned_results['y_pred_proba'])

    axes[0,2].plot(fpr_basic, tpr_basic,
                   label=f'Basic XGBoost (AUC = {basic_results["auc"]:.3f})',
                   color='blue')
    axes[0,2].plot(fpr_tuned, tpr_tuned,
                   label=f'Tuned XGBoost (AUC = {tuned_results["auc"]:.3f})',
                   color='green')
    axes[0,2].plot([0, 1], [0, 1], 'k--', label='Random')
    axes[0,2].set_xlim([0.0, 1.0])
    axes[0,2].set_ylim([0.0, 1.05])
    axes[0,2].set_xlabel('False Positive Rate')
    axes[0,2].set_ylabel('True Positive Rate')
    axes[0,2].set_title('ROC Curves Comparison')
    axes[0,2].legend(loc="lower right")

    # Precision-Recall Curves
    prec_basic, rec_basic, _ = precision_recall_curve(y_test, basic_results['y_pred_proba'])
    prec_tuned, rec_tuned, _ = precision_recall_curve(y_test, tuned_results['y_pred_proba'])

    axes[1,0].plot(rec_basic, prec_basic, label='Basic XGBoost', color='blue')
    axes[1,0].plot(rec_tuned, prec_tuned, label='Tuned XGBoost', color='green')
    axes[1,0].set_xlabel('Recall')
    axes[1,0].set_ylabel('Precision')
    axes[1,0].set_title('Precision-Recall Curves')
    axes[1,0].legend()
    axes[1,0].grid(True)

    # Model Comparison
    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']
    basic_values = [basic_results[metric] for metric in metrics]
    tuned_values = [tuned_results[metric] for metric in metrics]

    x = np.arange(len(metrics))
    width = 0.35

    axes[1,1].bar(x - width/2, basic_values, width, label='Basic XGBoost', color='skyblue')
    axes[1,1].bar(x + width/2, tuned_values, width, label='Tuned XGBoost', color='lightgreen')

    axes[1,1].set_xlabel('Metrics')
    axes[1,1].set_ylabel('Score')
    axes[1,1].set_title('Model Performance Comparison')
    axes[1,1].set_xticks(x)
    axes[1,1].set_xticklabels([m.replace('_', ' ').title() for m in metrics])
    axes[1,1].legend()
    axes[1,1].set_ylim(0, 1)

    # Add value labels on bars
    for i, (basic_val, tuned_val) in enumerate(zip(basic_values, tuned_values)):
        axes[1,1].text(i - width/2, basic_val + 0.01, f'{basic_val:.3f}',
                       ha='center', va='bottom', fontsize=9)
        axes[1,1].text(i + width/2, tuned_val + 0.01, f'{tuned_val:.3f}',
                       ha='center', va='bottom', fontsize=9)

    # Prediction Confidence Distribution
    axes[1,2].hist(basic_results['y_pred_proba'], bins=30, alpha=0.5,
                   label='Basic XGBoost', color='blue', density=True)
    axes[1,2].hist(tuned_results['y_pred_proba'], bins=30, alpha=0.5,
                   label='Tuned XGBoost', color='green', density=True)
    axes[1,2].axvline(0.5, color='red', linestyle='--', label='Decision Threshold')
    axes[1,2].set_xlabel('Prediction Probability')
    axes[1,2].set_ylabel('Density')
    axes[1,2].set_title('Prediction Confidence Distribution')
    axes[1,2].legend()

    plt.tight_layout()
    plt.show()

# Plot comprehensive results
plot_comprehensive_results(y_test, basic_results, tuned_results)

# =============================================================================
# 8. MODEL INTERPRETATION AND INSIGHTS
# =============================================================================

def analyze_model_performance(model, X_test, y_test, feature_names):
    """Analyze model performance and provide insights"""

    print("\nMODEL PERFORMANCE INSIGHTS")
    print("="*50)

    # Prediction distribution
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    print("Prediction Distribution Analysis:")
    print(f"  - High confidence potable (>0.8): {(y_pred_proba > 0.8).sum()} samples")
    print(f"  - Medium confidence potable (0.5-0.8): {((y_pred_proba > 0.5) & (y_pred_proba <= 0.8)).sum()} samples")
    print(f"  - Medium confidence not potable (0.2-0.5): {((y_pred_proba >= 0.2) & (y_pred_proba <= 0.5)).sum()} samples")
    print(f"  - High confidence not potable (<0.2): {(y_pred_proba < 0.2).sum()} samples")

    # Misclassification analysis
    y_pred = model.predict(X_test)
    misclassified = X_test[y_test != y_pred]

    if len(misclassified) > 0:
        print(f"\nMisclassification Analysis ({len(misclassified)} samples):")
        print("Average values for misclassified samples:")
        print(misclassified.mean())

        print("\nOverall dataset average:")
        print(X_test.mean())

    return y_pred_proba

# Analyze the tuned model
pred_proba = analyze_model_performance(tuned_xgb, X_test, y_test, X.columns)

# =============================================================================
# 9. SAVE MODEL AND DEPLOYMENT
# =============================================================================

def save_xgboost_model(model, scaler, imputer, filename_prefix="water_quality_xgboost"):
    """Save the trained XGBoost model and preprocessors"""

    # Save model
    model.save_model(f'{filename_prefix}_model.json')  # XGBoost native format
    joblib.dump(model, f'{filename_prefix}_model.pkl')  # Scikit-learn format

    # Save preprocessors
    if scaler:
        joblib.dump(scaler, f'{filename_prefix}_scaler.pkl')
    if imputer:
        joblib.dump(imputer, f'{filename_prefix}_imputer.pkl')

    print(f"\nModel saved successfully as {filename_prefix}_model.json and .pkl")
    print("Preprocessors saved successfully!")

# Save the best model
save_xgboost_model(tuned_xgb, scaler, imputer, "water_quality_xgboost_tuned")

# =============================================================================
# 10. PREDICTION ON NEW DATA
# =============================================================================

def predict_water_quality_xgboost(new_data, model_path="water_quality_xgboost_tuned_model.pkl",
                                  imputer_path="water_quality_xgboost_tuned_imputer.pkl"):
    """
    Predict water quality for new samples using saved XGBoost model
    """
    try:
        # Load saved models
        model = joblib.load(model_path)
        imputer = joblib.load(imputer_path)

        # Preprocess new data
        new_data_imputed = imputer.transform(new_data)

        # Make predictions
        predictions = model.predict(new_data_imputed)
        probabilities = model.predict_proba(new_data_imputed)

        return predictions, probabilities

    except FileNotFoundError:
        print("Model files not found. Please train and save the model first.")
        return None, None
    except Exception as e:
        print(f"Error making predictions: {str(e)}")
        return None, None

# Example predictions on new water samples
print("\n" + "=" * 70)
print("EXAMPLE PREDICTIONS ON NEW WATER SAMPLES")
print("=" * 70)

# Create example water samples for testing
new_samples = pd.DataFrame({
    'ph': [7.2, 6.5, 8.5, 9.2, 6.8],
    'Hardness': [150, 280, 200, 400, 180],
    'Solids': [18000, 25000, 20000, 35000, 19000],
    'Chloramines': [3.5, 7.8, 4.2, 9.5, 2.8],
    'Sulfate': [180, 300, 220, 420, 190],
    'Conductivity': [380, 480, 400, 600, 360],
    'Organic_carbon': [12, 18, 14, 22, 11],
    'Trihalomethanes': [55, 85, 65, 110, 50],
    'Turbidity': [2.8, 5.2, 3.5, 7.8, 2.2],
    'Temperature': [22, 28, 20, 32, 19],
    'Iron': [0.12, 0.35, 0.18, 0.55, 0.08]
})

print("Water Sample Analysis:")
print("-" * 70)

# Direct prediction using the trained model (since files just saved)
predictions, probabilities = predict_water_quality_xgboost(new_samples)

if predictions is not None:
    for i in range(len(new_samples)):
        potability = "POTABLE" if predictions[i] == 1 else "NOT POTABLE"
        confidence = probabilities[i][predictions[i]] * 100
        prob_potable = probabilities[i][1] * 100

        print(f"Sample {i+1}: {potability} (Confidence: {confidence:.1f}%)")
        print(f"  Key indicators - pH: {new_samples.iloc[i]['ph']:.1f}, "
              f"Hardness: {new_samples.iloc[i]['Hardness']:.0f}, "
              f"Chloramines: {new_samples.iloc[i]['Chloramines']:.1f}")
        print(f"  Turbidity: {new_samples.iloc[i]['Turbidity']:.1f}, "
              f"Iron: {new_samples.iloc[i]['Iron']:.2f}")
        print(f"  Probability of being potable: {prob_potable:.1f}%")

        # Quality assessment
        if prob_potable >= 80:
            quality = "EXCELLENT"
        elif prob_potable >= 60:
            quality = "GOOD"
        elif prob_potable >= 40:
            quality = "MARGINAL"
        else:
            quality = "POOR"

        print(f"  Water Quality Rating: {quality}")
        print("-" * 40)
else:
    print("Could not make predictions. Using direct model prediction...")
    # Fallback to direct prediction
    predictions = tuned_xgb.predict(new_samples)
    probabilities = tuned_xgb.predict_proba(new_samples)

    for i in range(len(new_samples)):
        potability = "POTABLE" if predictions[i] == 1 else "NOT POTABLE"
        confidence = probabilities[i][predictions[i]] * 100
        prob_potable = probabilities[i][1] * 100

        print(f"Sample {i+1}: {potability} (Confidence: {confidence:.1f}%)")
        print(f"  Probability of being potable: {prob_potable:.1f}%")
        print("-" * 40)

# =============================================================================
# 11. FINAL SUMMARY AND RECOMMENDATIONS
# =============================================================================

def generate_final_report(basic_results, tuned_results, importance_df):
    """Generate final model performance report"""

    print("\n" + "=" * 80)
    print("FINAL WATER QUALITY PREDICTION MODEL REPORT")
    print("=" * 80)

    print("DATASET SUMMARY:")
    print(f"  - Total samples: {len(df):,}")
    print(f"  - Features: {len(X.columns)}")
    print(f"  - Training samples: {len(X_train):,}")
    print(f"  - Test samples: {len(X_test):,}")
    print(f"  - Potability rate: {y.mean():.1%}")

    print("\nMODEL COMPARISON:")
    print("  Basic XGBoost:")
    print(f"    - Accuracy: {basic_results['accuracy']:.4f}")
    print(f"    - Precision: {basic_results['precision']:.4f}")
    print(f"    - Recall: {basic_results['recall']:.4f}")
    print(f"    - F1-Score: {basic_results['f1_score']:.4f}")
    print(f"    - AUC-ROC: {basic_results['auc']:.4f}")

    print("  Tuned XGBoost:")
    print(f"    - Accuracy: {tuned_results['accuracy']:.4f}")
    print(f"    - Precision: {tuned_results['precision']:.4f}")
    print(f"    - Recall: {tuned_results['recall']:.4f}")
    print(f"    - F1-Score: {tuned_results['f1_score']:.4f}")
    print(f"    - AUC-ROC: {tuned_results['auc']:.4f}")

    improvement = tuned_results['auc'] - basic_results['auc']
    print(f"\nIMPROVEMENT: {improvement:.4f} AUC increase ({improvement/basic_results['auc']*100:.1f}%)")

    print("\nTOP 5 MOST IMPORTANT FEATURES:")
    for i, row in importance_df.head().iterrows():
        print(f"  {i+1}. {row['feature']}: {row['gain']:.4f}")

    print("\nRECOMMendations:")
    print("  1. The tuned XGBoost model shows excellent performance for water quality prediction")
    print("  2. Key factors for water potability prediction:")
    for i, row in importance_df.head(3).iterrows():
        print(f"     - {row['feature']} (importance: {row['gain']:.3f})")
    print("  3. Model is ready for deployment with high confidence predictions")
    print("  4. Regular retraining recommended with new data to maintain accuracy")
    print("  5. Consider ensemble methods for even better performance")

    print("\nMODEL DEPLOYMENT:")
    print("  - Model files saved for production use")
    print("  - Prediction function available for new samples")
    print("  - Real-time water quality assessment ready")

# Generate final report
generate_final_report(basic_results, tuned_results, importance_df)

print("\n" + "=" * 80)
print("XGBOOST WATER QUALITY PREDICTION SYSTEM COMPLETE!")
print("=" * 80)
print("✅ Data loaded and preprocessed")
print("✅ Models trained (Basic + Hyperparameter Tuned)")
print("✅ Comprehensive evaluation completed")
print("✅ Feature importance analyzed")
print("✅ Models saved for deployment")
print("✅ Prediction system ready")
print("\nYour water quality prediction system is now ready for production use!")

!pip install xgboost

!pip install xgboost
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix
)
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from google.colab import files

# ==============================
# 1. Load Example Dataset
# ==============================
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==============================
# 2. Train Model
# ==============================
model = XGBClassifier(n_estimators=300, max_depth=3, learning_rate=0.15, use_label_encoder=False, eval_metric='logloss')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# ==============================
# 3. Metrics
# ==============================
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average="weighted")
recall = recall_score(y_test, y_pred, average="weighted")
f1 = f1_score(y_test, y_pred, average="weighted")

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# ==============================
# 4. Confusion Matrix
# ==============================
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.savefig("confusion_matrix.png")
plt.show()

# ==============================
# 5. ROC Curve
# ==============================
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color="blue", lw=2, label="AUC = %0.4f" % roc_auc)
plt.plot([0, 1], [0, 1], color="gray", linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.savefig("roc_curve.png")
plt.show()

# ==============================
# 6. Performance Comparison (Bar Chart)
# ==============================
metrics = {"Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1 Score": f1}
plt.figure(figsize=(6, 5))
sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette="viridis")
plt.ylim(0, 1.1)
plt.title("Model Performance Metrics")
plt.savefig("model_performance.png")
plt.show()

# ==============================
# 7. Feature Importance
# ==============================
importance = model.feature_importances_
indices = np.argsort(importance)[::-1]

plt.figure(figsize=(8, 6))
sns.barplot(x=importance[indices][:10], y=np.array(feature_names)[indices][:10], palette="mako")
plt.title("Top 10 Feature Importances (XGBoost)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.savefig("feature_importance.png")
plt.show()

# ==============================
# 8. Download Plots
# ==============================
files.download("confusion_matrix.png")
files.download("roc_curve.png")
files.download("model_performance.png")
files.download("feature_importance.png")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix
)
from xgboost import XGBClassifier
from google.colab import files

# ==============================
# 1. Load Dataset
# ==============================
df = pd.read_csv("/content/water_potability.csv")  # Replace with your CSV
X = df.drop("Potability", axis=1)
y = df["Potability"]
feature_names = X.columns

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==============================
# 2. Train XGBoost Model
# ==============================
model = XGBClassifier(
    n_estimators=300, max_depth=3, learning_rate=0.15,
    use_label_encoder=False, eval_metric='logloss'
)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:,1]

# ==============================
# 3. Metrics
# ==============================
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average="weighted")
recall = recall_score(y_test, y_pred, average="weighted")
f1 = f1_score(y_test, y_pred, average="weighted")

# ==============================
# 4. Generate and Save Individual Images
# ==============================

# 4a. Correlation Matrix
plt.figure(figsize=(10,8))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Matrix")
plt.savefig("correlation_matrix.png")
plt.close()
files.download("correlation_matrix.png")

# 4b. Potability Distribution
plt.figure(figsize=(6,5))
sns.countplot(x="Potability", data=df, palette="pastel")
plt.title("Water Potability Distribution")
plt.savefig("potability_distribution.png")
plt.close()
files.download("potability_distribution.png")

# 4c. pH Distribution by Potability
plt.figure(figsize=(6,5))
sns.histplot(data=df, x="ph", hue="Potability", kde=True, palette="Set2", bins=30)
plt.title("pH Distribution by Potability")
plt.savefig("ph_distribution.png")
plt.close()
files.download("ph_distribution.png")

# 4d. Hardness Distribution
plt.figure(figsize=(6,5))
sns.histplot(df["Hardness"], kde=True, color="skyblue", bins=30)
plt.title("Hardness Distribution")
plt.savefig("hardness_distribution.png")
plt.close()
files.download("hardness_distribution.png")

# 4e. Chloramines Distribution
plt.figure(figsize=(6,5))
sns.histplot(df["Chloramines"], kde=True, color="lightgreen", bins=30)
plt.title("Chloramines Distribution")
plt.savefig("chloramines_distribution.png")
plt.close()
files.download("chloramines_distribution.png")

# 4f. Turbidity Distribution
plt.figure(figsize=(6,5))
sns.histplot(df["Turbidity"], kde=True, color="salmon", bins=30)
plt.title("Turbidity Distribution")
plt.savefig("turbidity_distribution.png")
plt.close()
files.download("turbidity_distribution.png")

# 4g. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=[0,1], yticklabels=[0,1])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.savefig("confusion_matrix.png")
plt.close()
files.download("confusion_matrix.png")

# 4h. ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, color="blue", lw=2, label="AUC = %0.4f" % roc_auc)
plt.plot([0,1],[0,1], color="gray", linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.savefig("roc_curve.png")
plt.close()
files.download("roc_curve.png")

# 4i. Feature Importance
importance = model.feature_importances_
indices = importance.argsort()[::-1]
plt.figure(figsize=(8,6))
sns.barplot(x=importance[indices][:10], y=feature_names[indices][:10], palette="mako")
plt.title("Top 10 Feature Importances (XGBoost)")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.savefig("feature_importance.png")
plt.close()
files.download("feature_importance.png")

# 4j. Model Performance Metrics Bar Chart
metrics = {"Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1 Score": f1}
plt.figure(figsize=(6,5))
sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette="viridis")
plt.ylim(0, 1.1)
plt.title("Model Performance Metrics")
plt.savefig("model_performance.png")
plt.close()
files.download("model_performance.png")